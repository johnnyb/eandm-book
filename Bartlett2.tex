\chapter{Calculating Software Complexity Using the Halting Problem}
\author{Jonathan Bartlett}

\begin{abstract}
Calculating the complexity of software projects is important to software engineering as it helps in estimating the likely locations of bugs as well as the amount of resources required to modify certain program areas.  Cyclomatic complexity is one of the primary estimators of software complexity which operates by counted branch points in software code.  However, cyclomatic complexity assumes that all branch points are equally complex.  Some types of branch points require more creativity and foresight to understand and program correctly than others.  Specifically, when knowledge of the behavior of a loop or recursion requires solving a problem similar to the halting problem, that loop has intrinsically more complexity than other types of loops or conditions.  Halting-problem-like problems can be detected by looking for loops whose termination conditions are not intrinsically bound in the looping construct.  These types of loops are counted to find the program complexity.  This metric is orthogonal to cyclomatic complexity (which remains useful) rather than as a substitute for it.
\end{abstract}

\section{Complexity Metrics in Software}

Managing software development is often about managing risks - knowing which tasks are likely to take more time than others, which features are more likely to impact others, how much testing will be required to make sure that a feature is solid, and whether a bug fix or a feature implementation requested right before release will be more likely to make the code more stable or lead to other bugs.

One of the key considerations of risk management is software complexity.  Complex software is inherently more difficult to build, test, and maintain.  Therefore, it is critical for software development managers to know which parts of code are most complex and therefore more likely to incur failures if modified.

\section{A Brief History of Software Complexity Metrics}

Early complexity metrics were based almost entirely on the amount of code produced.  Therefore, a function which contained 10 lines of code was considered more complex than one which contained only 5.  However, since ``lines'' of code is often includes stylistic approaches, Halstead developed a set of measures based on the number of operators and operands present within the code.\cite{kearney}  

Lines of code and related metrics are still often used for software effort estimation, but its use in analyzing code complexity has fallen away.  It quickly became clear that not all code is the same, and some operators are inherently more complex than others.  Specifically, the decision structure of the program was inherently more complex than the computations.  Cyclomatic complexity was created to measure the size of the decision structure of the program.\cite{mccabe}  This is done by creating a graph of all basic code blocks as nodes, and then edges which show how control can move between them.  The formula for calculating the complexity is:

$$E - N + T$$

$E$ is the number of edges, $N$ is the number of nodes, and $T$ is the number of terminating nodes (entry and exit points - usually 2).   

Cyclomatic complexity is extremely useful in determining how to test software.  The cyclomatic complexity of a program is also the minimum number of tests needed to cover every control flow branch of a program.  If the cyclomatic complexity of a program is 5, then at least 5 tests must be devised to test every branch of code.  Such tests do not guarantee total coverage of all possible test conditions, but they will verify that every statement in the program will contribute to at least one test.

The ABC metric is a simplified metric combining aspects of both lines of code and cyclomatic complexity.  It works by simply counting the number of assignment statements, the number of branches (direct flow control shifts - i.e. function calls), and the number of conditionals.\cite[pg.~2--3]{fitzpatrick}  These can then be analyzed on a whole program, per-module, or per-function basis, to give an overview of complexity and size of a software program.

\section{Deeper Difficulties in Software}

While each of the previously-mentioned metrics have their usefulness, none of them get at the deeper difficulties that make software projects complex.  Many programming languages have attempted to remove the inherent difficulties within software.  Some, like COBOL, attempted to remove the mathematical notation common to programming languages.  Others, like Java, attempt to simplify software by encapsulating related methods into objects.  Visual programming languages, such as Flowcode, turn all software into visual representations, allowing the user to drag and drop flowchart-like components to accomplish programming tasks.  The idea is that it is the text-based nature of the software which causes complexity and confusion within software.

While each of these actually do relieve certain specific problems in software development, none of them are able to remove the complexity of software development, because the complexity is inherent in the nature of software development itself.  What makes software difficult is their open-ended nature.  Most general-purpose programming languages today are Universal in nature - that is, they can perform any computable function that any other programming language can perform.  Thus, they are open-ended - the types of operations that they perform are entirely specifiable by the programmer, and are not restricted.

Universal programming languages are chaotic - that is, there is no easy mapping between programs, data, and results.  Therefore, predicting the output over a wide swath of code and data can be difficult.  

So what parts of a language cause a language to be chaotic?  Arbitrary looping structures.  Interestingly, this is precisely the same part that causes it to be open-ended.  Arbitrary looping structures allow a programmer to generate any possible computable function.  As such, they also allow a programmer to write programs whose results are chaotic.  In practical terms, arbitrary looping structures are \verb+while+ statements, \verb+jump+/\verb+goto+ statements, recursive functions, and continuations, though others may be possible.

Occasionally, the solution to this problem has been to reduce the scope of the language.  SuperGlue is one language which is specifically designed to be as expressive as possible while avoiding constructs that lead to complexity.\cite{mcdirmid}  However, ultimately, to get beyond the originally conceived computational bounds of the programming language, Universality, as well as the complexity that goes with it, are required.

\section{The Halting Problem as an Insight Problem}

As discussed elsewhere in this volume, some problems are not amenable to analytical analysis, and require insight in order to solve them.\cite{bartlett1}\cite{holloway}  Neither are such problems computable - no computer is capable of calculating these problems.  One such problem that is relevant for us is the halting problem.

The halting problem states that, given a Universal language, there is no program that can be written which will tell if any arbitrary program written in that language will ever complete (i.e. halt).  This is not based on the size of the program code, but rather of the nature of the constructs available.  This is not to say that one couldn't write a program to tell if certain subsets of programs written in that language will halt, but there could not be a program to tell if any given program would halt.

The interesting thing about this, as noted by Bartlett\cite{bartlett1}, is that computer programmers seem to be able to possess this power to some degree.  Since the complexity of problems assigned to them are arbitrarily hard (i.e. management, not the programmer, often decides what must be done), and the reason that arbitrarily hard programs are possible is because of the open-ended nature of Universal programming languages (the parts of the language which *create* the chaos are precisely the ones *required* for programs of arbitrary complexity), we can say that human programmers are generally reliable halting problem solvers.

There are cases (usually coming from number theory) where programs (even very simple ones!) are not known whether or not they halt.  These are interesting, and it certainly seems that there are different levels of hardness and different levels of insight required to make determinations.  Nonetheless, the advancement of science and mathematics actually depends on the ability of humans to be able to accomplish this.  In other words, if we doubt the ability of humans to figure out such problems, then we should cast doubt on the progress of science itself.  Should we tell mathematicians to stop looking for answers in number theory?   Or should we assume that the insight will come one day?  I argue that the ability of programmers to reliably solve halting problems in their daily work should lend hope to the mathematicians that someone will be able to have the insight to solve their problems as well.

\section{Using the Halting Problem to Measure Software Complexity}

The trouble with insight problems is that they are not reliably solved by individuals.  They are unreliably solvable - in other words, we can trust that a solution is possible, but we can't rely on an analytic procedure to do so.  Even worse, if a programmer doesn't realize that they are looking at an insight problem, they might not know that special care must be taken.

So how can we recognize an insight problem in code?  Since we have already determined the types of programming structures which make a program universal (arbitrary looping structures), we can therefore detect those structures in code.

For most programming languages, the main structures which must be detected are:

\begin{enumerate}
\item Loops where the iterations are not implicit in the control structure
\item Recursive functions (which are just another way of implementing \#1)
\end{enumerate}

For \#1, consider the following two programs that each print out the square of every number in an array:

\begin{figure}[H]
\begin{verbatim}
ary.each{|x| puts x * x}
\end{verbatim}
\caption{A program with an implicitly-terminating loop}
\label{fig:impterm}
\end{figure}

\begin{figure}[H]
\begin{verbatim}
i = 0
while(i < a.length) {
	puts x * x
	i = i + 1
}
\end{verbatim}
\caption{A program with an arbitrary looping structure}
\label{fig:expterm}
\end{figure}

The implementation in Figure \ref{fig:expterm} is more complex than the one in Figure \ref{fig:impterm}, but not because of the size of the program.  What makes it more complex is that it utilizes an arbitrary looping structure - the while statement.  In Figure \ref{fig:impterm}, the looping in inherently bound by the loop operator.  In Figure \ref{fig:expterm}, the programmer must specifically act to make the loop terminate appropriately.  There is no way an \verb+each+ statement on its own will fail to halt.  There are many ways in which a while statement can fail to halt.  The termination is decoupled from the loop construct itself.

Therefore, as a first pass to measure software complexity, we will simply count the number of open-ended looping structures (either as loops or recursive functions) which occur in the program.   

\section{Adding Axioms to Minimize Insight}

However, as is obvious from the simple example above, there are many well-understood conventions which mitigate the complexity of certain kinds of open-ended looping structures.  In the example above, starting at zero, monotonically increasing, and then terminating at a predetermined stopping point, while it may not be codified within the language, is a well-understood looping convention that, if followed correctly, will result in the loop's termination.

Chaitin formalized this idea in his algorithmic information theory.  He pointed out that while certain problems were unsolvable given a base set of axioms, by incorporating additional axioms into the problem, solutions can be found.  For instance, following Chaitin, if God were to tell us how many programs of size N halted, we could use that information to solve the halting problem for programs of size N.\cite{chaitin}\footnote{For a proof of this, consider that the issue that makes the halting problem difficult is that if you follow the execution of the program, the ones which don't halt will never finish, but we won't ever \emph{know} that it will never finish, since at any moment it may still finish.  However, if we know that X programs of size N will halt, you can simply run all programs of size N simultaneously.  Then, we can wait until those X programs finish, and we will know that the rest of the programs will never finish.  Therefore, we know that we can know the answer to all the halting problems in a finite length of time - the length of time will be the maximum runtime of the longest-running halting program.}

In the same way, when a convention for constraining repetition is discovered, it can be incorporated into a canon of axioms which are also known to halt.  And thus it should be treated almost on the same level as a language construct which produces close-ended loops.  I say ``almost'' because language constructs enforce the validity of the axiom by force, while conventions require that the programmer follow the convention correctly.

This cannon of axioms can be codified into an extensible static analysis tool to check program complexity.  Such a tool could consist of the set of potential non-terminating constructs, as well as a ``book of conventions'' which are the known conventions for ensuring termination.  The tool would then measure the potential number of non-terminating constructs which do not conform to a pattern in the ``book of conventions.''\footnote{A similar procedure was independently developed by Bringsjord et al\cite{bringsjord}, Hertel\cite{hertel}, and Harland\cite{harland}, though differing in many aspects and applications.  Their solutions were to categorize non-halters rather than halters, and to do it based on runtime patterns rather than a static analysis of structural patterns in the program.  They identified well-known patterns, data-mined for others, and then used a symbolic induction prover to match potential programs with these patterns. In addition, their purpose was for answering questions about computer science theory (specifically, the busy beaver problem) rather than assessing program complexity.}

In addition to the constructs which can be statically analyzed, some conventions (often termed as ``patterns'') will not be easily amenable to inference by software.  They should, however, at least be documented, and they can be manually marked or removed after-the-fact.  However, if the construct is not amenable to static analysis, extra effort should be taken to review all implementations of the construct manually.

It should also be recognized that these axioms should be treated as first-class insights.  That is, the ``book of conventions'' should be considered a set of valuable intellectual assets.  As solutions to insight problems, the ``book of conventions'' is by definition a set of solutions which are not immediately obvious, and, therefore, if a convention is not recorded, and is therefore ``lost'', it could well be a permanent loss of insight for an organization.

\section{Using the Metric}

The ultimate goal of the metric is to reduce the complexity of the software to zero.  Remember, when a pattern which solves a restricted subset of the halting problem is discovered, it can be incorporated as a new axiom into the ``book of conventions''.  Therefore, if there are any areas in the program which are marked as being complex, that means that it is still not known if the program will even finish!  If a developer has a new insight into why a certain section of code will finish, this should be documented in the ``book of conventions''.  If the programmer cannot state why they think that the program will finish, it should be reviewed or rewritten.  If the code cannot be reworked, and the program cannot be proven to terminate, it should be considered highly suspicious.

In addition, areas of code which are complex given the constructs, but found in the ``book of conventions'' should be flagged for a second-pass review, to make sure the conventions were followed appropriately.  Also, those sections should also be flagged for programmers making modifications, to be sure that their modifications do not upset the assumptions of the conventions.

\section{Further Considerations}

While this metric is very useful, it is obviously not the last word on complexity metrics.  It does not technically supersede the other complexity metrics mentioned.  Lines of code is still a useful planning tool.  Cyclomatic complexity is still a useful test coverage tool.  However, this metric can be useful in identifying programming patterns which are intrinsically problematic, and help mitigate possible problems with documentation, code review, and testing.  

For future development, similar ideas could be applied not just to the halting complexity, but also to the complexity that variables are derived from.  When the value of a variable is determined by multiple loops, or conditions within loops, or other sorts of non-linear mechanisms, the value of variables can be chaotic, even when they are not themselves what determines if the problem halts.  Extending these ideas to variable calculation could allow for an even more comprehensive look at where program complexity lies.

\bibliographystyle{plainnat}
\bibliography{Bartlett2Library}


