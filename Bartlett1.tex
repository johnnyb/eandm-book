\chapter{Using Turing Oracles in Cognitive Models of Problem--Solving}

\begin{abstract}
At the core of engineering is human problem--solving.  Creating a cognitive model of the task of problem--solving is helpful for planning and organizing engineering tasks.  One possibility rarely considered in modeling cognitive processes is the use of Turing Oracles.  \citet{copeland_1998} put forth the possibility of that the mind could be though of as an oracle machine, but never applied that idea practically.  Oracles enable the modeling of processes in the mind which are not computationally--based.  Using oracles resolves many of the surprising results of computational problem--solving which arise as a result of the Tractable Cognition Thesis and similar mechanistic models of the mind.  However, as research into the use of Turing Oracles in problem--solving is new, there are many methodological issues.
\end{abstract}

\section{Broad Views of Cognition and Their Historic Consequences in Cognitive Modeling}

There are three main contenders in the philosophy of mind for how to understand cognitive processes in the broadest view.  Physicalism is the idea that there is nothing going on in the mind that isn’t describable through standard physical processes.  There may yet be physical processes that we don’t currently understand or are even aware of, but, in the long run, there should not be anything involved in causal processes that is not physical and understandable through physics.  Some physicalists allow for the non--reduction of mental states to physical states, or at least an epistemological reduction, but they are all clear in the closed causality of the physical \citep{horgan_1994}.

Dualism is the primary contender for this area.  Dualism is the idea that the mind and the body are not equivalent --- that there exists at least some part of human cognition that is beyond what is describable by physics or using physical entities.  It holds that a reduction of the mind to brain physics does not capture the entirety of what the mind is doing.  It also says that there is a causal element being left out --- that the mind, while not itself entirely physical, participates in the causal chain of human action.  In other words, it is not a purely passive element, but has a causal role \citep{hart_1994}.

A third contender is emergentism.  Emergentism tries to split the line between physicalism and dualism.  However, it is a very soft term and is a hard one to distinctly identify.  Some forms of emergentism (especially ``weak emergence'' or ``epistemological emergence'') are essentially physicalism, while others (for instance, ``strong emergence'' or ``ontological emergence''), propose outside laws of emergence which transform the character of initial properties in certain configurations \citep{oconnor_wong_2012}.  Therefore, strong emergence tends to be essentially a form of dualism, except that the dualistic properties are captured in a set of laws of emergence.  The question then is whether these laws themselves can be considered material.  Since our question is about whether or not physical causation is sufficient for explanation, most views of emergence can be classified as either physicalist or dualist for the purposes of this paper.

To what extent do these broad views of cognition affect theory development?  Historically, aspects of cognition that were considered to be part of the non--physical mind were left unmodeled by dualists.  By contrast, the goal of physicalism is to force all phenomena into explicitly physical models.  But what constitutes a physical model?  This question is much harder than it seems.

Most people considering the question of physicalism and dualism tend to have a hard time pinning down what exactly physicalism is.  For a dualist to say that there is more than one mode of causation, it needs to be fairly explicit about at least one of those modes.  Similarly, if a physicalist says that all causes are physical, such a statement is meaningless without a solid definition of what counts as physical and what doesn’t \citep{stoljar_2009}.

In most discussions, several unhelpful definitions of physicalism are often proposed.  One of the most often used definitions is that physicalism deals only with material causes.  But this just pushes the question back --- what counts as a physical cause?  Another definition is that physicalism deals only with observable phenomena.  This could have two meanings, both of which are problematic.  If it means that it deals only with things which can be observed directly, then this would remove most of modern physics from the physical -- direct observation is not possible for atoms, molecules, forces, and the like.  If, on the other hand, the definition includes indirect observations, then there is no reason to suppose that only physical entities are observable.  It is precisely the contention of the dualists that there are non--physical modes of causation which have real effects in the world.  If dualism is true, then non--physical causes should be indirectly observable.  Therefore, observability can’t be a distinguishing factor.  A third definition is that physical things are testable.  However, this fails for the same reason that the observable definition fails.  Testing simply means looking at observations, and determining whether or not they match the expectations of the theory.  Therefore, any observable phenomena should be testable in the same way.

So what can be used to distinguish physical behavior from non--physical behavior?  There is one distinguishing factor which has been proposed by physicalists to describe what they mean by physical which can be a useful distinguisher between physical and non--physical behavior --- computability.  On this definition, physical processes are those whose results can (at least in theory) be calculated by computational systems, while non--physical processes are those which cannot.  This has been proposed by Iris van Rooij in his Tractable Cognition Thesis as well as Stephen Wolfram in his Principle of Computational Equivalence \citep{van_rooij_2008, wolfram_2002}.  This definition has the benefits of a well--defined theory of computability and incomputability which was defined in the early 20th century by Gödel, Church, Turing, and others.  As such, it becomes at least possible to make meaningful statements about physical and non--physical processes.

In addition, as we will see, because of the groundwork laid by the same pioneers of computability and incomputability, we will be able to go beyond previous dualist conceptions of the mind, and actually include non--physical elements in our models of cognition.

\section{A Primer on Computability and Incomputability}

The concept of incomputability is not intuitive for most people.  How can a function not be computable?  Incomputability generally refers to the question of whether or not a given function can be computed given a set of operators.  So, for instance, given only the addition, subtraction, and summation operators, division cannot be computed.  However, given those same operators, a multiplication function can be computed.  

One place where incomputability reigns is on self--referential questions.  There are numerous questions that one can ask, and even answer, about a set of mathematical operators which cannot be answered solely by the functions of the operators themselves.  For instance, let’s say you have a set of operators O and a list L of all of the valid functions that take a single value as a parameter, give a single value as a result, are of finite length, and can be defined using the operators in O, listed in alphabetic order.  This is a countable infinity, because each function in L can be identified by an ordinal number, which is its index into the list.  Now, because all of L are valid functions, there exists a function F(x) which takes the function from L at index x, and returns the value of that function with x as the parameter.  The question is, is F(x) in L?  

The answer, perhaps surprisingly, is no.  This means that defining F(x) will require operators not in O.  How is this shown?  Well, let’s take another function, G(x), which returns F(x) + 1.  If F(x) is in L, then, G(x) is also in L (assuming that the addition operator is in O).  Now, let’s say G(x) is at index n of L and has a result r when computed using its own index (which we have defined to be n).   Now, by definition, since n is the index of G, then F(n) must return the same result as G(n), which we have defined to be r.  However, the definition of G(x) says that it must be F(x) + 1!  Since F(n) returns r and G(n) returns r, this leads to a contradiction, because r cannot be equal to r + 1.  Therefore, F(x) cannot be computed using the operators in O. As you can see, this proof is quite independent of what operators exist in O, provided they are singly--valued and include the addition operator.  Thus, F(x) is a true function of x, but is incomputable with fixed--length programs of operators in O.

Now, based on the example given above, it seems that computability questions are based on the set of operators being used to define it.  This is largely true.  So, if the question of computability is operator--dependent, how can a notion of computability help us in the case of answering questions about the physicality of the process?  The Church--Turing thesis answers this.  The Church--Turing thesis states that all finitary mathematical systems are computationally equivalent to some Turing machine \citep{turing_1939}.    

The Church--Turing thesis was discovered when several finitary logic systems were developed independently, including Church’s lambda calculus.  It is hard to imagine two systems so different in approach as Church’s lambda calculus and the Turing machine.  Yet, in the end, it was proven that they have the exact same computational abilities.  Now, to be technically correct, especially with Turing machines, it is their maximum abilities which are equivalent.  One can define a Turing machine with equivalent or less computational power than the lambda calculus, but not with more.  Thus, the computational power of finitary systems do imply a fixed set of operators.  

Such finitary systems which have this maximal computational power are known as universal machines, or universal computation systems, since they can be programmed to perform any calculation that is possible on a finitary computation system.  Thus, any computability question that would be true for one of them would be true for all of them.  Therefore, when used without qualification, incomputability usually refers to something which is incomputable on a universal computation system.

Wolfram and van Rooij both use universal computation to set a maximal level of sophistication available in nature.  Here is how Wolfram phrases it for his Principle of Computational Equivalence:

\begin{quote}
One might have assumed that among different processes there would be a vast range of different levels of computational sophistication.  But the remarkable assertion that the Principle of Computational Equivalence makes is that in practice this is not the case, and that instead there is essentially just one highest level of computational sophistication, and this is achieved by almost all processes that do not seem obviously simple...For the essence of this phenomenon is that it is possible to construct universal systems that can perform essentially any computation --- and which must therefore all in a sense be capable of exhibiting the highest level of computational sophistication \citep[][pg. 717]{wolfram_2002}.
\end{quote}

Wolfram is thus stating that within nature, computability is the limiting factor of what is possible.  Van Rooij, while restricting his comments to the nature of the mind, makes basically the same point:

\begin{quote}
human cognitive capacities are constrained by computational tractability.  This thesis, if true, serves cognitive psychology by constraining the space of computational--level theories of cognitions \citep{van_rooij_2008}
\end{quote}

In other words, if the brain is constrained by computational tractability, then it limits the possible set of models which could be used when modeling cognition.  Van Rooij specifically traces this back to the idea that the Church--Turing thesis is not merely a limitation of finitary computation, but is a limitation of reality as a whole, or, as van Rooij puts it, ``the Church--Turing Thesis is a hypothesis about the state of the world'' \citep{van_rooij_2008}.

Wolfram similarly applies his ideas specifically to the brain, saying:

\begin{quote}
So what about computations that we perform abstractly with computers or in our brains?  Can these perhaps be more sophisticated?  Presumably they cannot, at least if we want  actual results, and not just generalities.  For if a computation is to be carried out explicitly, then it must ultimately be implemented as a physical process, and must therefore be subject to the same limitations as any such process. \citep[][pg. 721]{wolfram_2002}
\end{quote}

So, as you can see, the definition of physicalism, so far that it is defined sufficiently so as to be distinguishable from anything else, has been defined by its supporters as being equivalent to computationalism.  This allows us to then take a more methodical look at these conceptual models to determine if they are likely to be true.

\section{The Halting Problem}

One of the classic unsolvable problems in computability is the ``halting problem.''  In universal computation systems, there are ways to cause computations to repeat themselves.  However, this leads to a possible problem --- if a function is poorly written, the function may get caught in a repetitive portion, and not be able to leave.  This computation would be a non--halter, and therefore, left to itself, would never complete.  Most familiar computations are halting computations.  So, take the following computer program (all programming examples are given in Javascript for readability):

\begin{verbatim}
function double(x) {
	var y;
	y = x * 2;
	return y; 
}
\end{verbatim}

This program defines a function called ``double'' which doubles its input.  It creates a temporary variable called ``y'' to hold the result of the computation, and then returns y as the final value for the function.  So, after defining it, I can then use the function by saying double(4) which would give 8, or double(z) which would take the value currently denoted by z and return whatever is double of z.

Now let’s look at a program with a loop in it.  This program computes the factorial of a number.  The factorial of a number is the result of multiplying a number by all of the numbers below it down to 1.  For instance, factorial(5) is 5 * 4 * 3 * 2 * 1.  factorial(3) is 3 * 2 * 1.  So, as you can see, the number of computations performed, while always finite for a finite number, varies with the value given.  Here is a typical way to program a factorial function:

\begin{verbatim}
function factorial(x) {
	var val;
	var multiplier;

	val = 1;
	multiplier = x;

	while(multiplier > 1) {
		val = val * multiplier;
		multiplier = multiplier - 1;
	}

	return val; 
}
\end{verbatim}

This function defines two temporary variables --- val, which holds the present state of the computation, and multiplier, which holds the next number that needs to be multiplied.  Note that unlike algebraic systems, in most computer programming languages, variables do not have static values, but can change over the course of the program.  The ``='' is not an algebraic relationship, but rather it means assignment.  val = 1 means that we are assigning the value 1 to the variable val.  

So, in this program, we start off with the multiplier set to the number given.  Then, we enter the loop.  This says, while the value in the multiplier variable is greater than 1, perform the given computation.  If we perform the function with the value of 3, multiplier will start with 3, which is greater than 1.  Then it will perform the computation --- it will multiply val (which starts off at 1) with 3, and then assign that result back into val.  val now has the number 3.  multiplier is then decreased by one, and now has the value 2.  The bracket indicates the end of our loop, so the condition is re--evaluated.  multiplier’s value of 2 is still greater than one, so we perform the loop again.  val (which is now 3) is multiplied by multiplier (which is now 2) and the value (6) is assigned back into val.  multiplier is again decreased and is now 1.  Now when we evaluate the condition, we see that multiplier is no longer greater than 1.  Because the condition is no longer true, the loop does not run again, and it goes on to the next statement.  

The next statement tells us to return the value in val as the result of the entire computation.  Thus, since val currently holds 6, this returns 6 as the result of factorial(3), which is the correct result.  Since it does eventually return a value, it is considered a halting program.  Note that it will take longer to return a value if the input is bigger, and it will return invalid values if the input is less than one or not an integer, but it will always return a value.  Therefore, since it will always complete in a finite number of steps, it is a halter.

Now, let’s say that the person writing this function forgot a step.  Let’s say that they forgot to write the instruction that decreases the multiplier.  So, instead of the previous program, they wrote the following:

\begin{verbatim}
function factorial(x) {
	var val;
	var multiplier;

	val = 1;
	multiplier = x;
	while(multiplier > 1) {
		val = val * multiplier;
	}

	return val; 
}
\end{verbatim}

Since multiplier is never decreased, for any input greater than 1, this function will never stop computing!  Therefore, in terms of the halting problem, it doesn’t halt.

Now, another interesting fact about functions on universal computation systems is that they are convertible to numbers.  In fact, that’s how computers work -- the computer stores the program as a very large number.  As an example for how this might work, suppose that each character in the above program can be converted to a number, and then joined together to a large number to denote the program.  And this is, in fact, how some programming languages work.  Most of the time, however, the conversion of a program into a number actually works by doing a more intensive conversion of the program into a numeric language that the computer understands.  

Nonetheless, in each case, the program gets converted into a (usually very large) number.  Therefore, since any program can be converted into a counting number, this means both that there is only a countably infinite number of possible programs.  But even more important for our purposes, it means that this program, since it is (or can be represented by) a number, can itself be an input to a function!

Now, some functions halt on certain inputs, but do not halt on other inputs.  So, the halting question can only be asked on a combination of both the program and the input, since some inputs may halt and others may not.  So, the halting problem is a function of two variables -- the program p and the input i.  So, now, every program/input combination either will halt or it will not.  There is no in--between state possible on finitary computations.  Therefore, we can denote H(p, i) as a function which takes the program p and input i gives as a result a 1 if p(i) halts, or a 0 if p(i) does not halt.  This is known as a ``decision problem'' --- a problem which takes inputs and decides if the inputs have a feature or match a given pattern.  So the question is, can we write a program to perform H(p) using a universal computation system?  The answer is no.  This can be proved similarly to the early proof of incomputability.  

Let’s first assume that H(p, i) is a program that can be implemented with a universal computation system.  If H(p, i) can be implemented, then it can also be used by a longer program.  So, therefore, let’s create a program called N(p).  This program looks like the following:

\begin{verbatim}
function N(p) {
	if(H(p, p) == 1) {
		while(1 == 1) {
		}
	}

	return 1; 
}
\end{verbatim}

This function starts by evaluating the halting problem of its input, p.  If the halting problem of a program p with itself as the input says ``Yes it halts'' (i.e. it gives a value of 1), then perform an infinite loop (i.e. don’t halt).  Otherwise, finish the computation returning a value of 1 (i.e. do halt).  Now, let’s ask the question, does N(N) halt?  If it does, then this program will loop forever, but it can’t, because we have already determined that it does not halt!  Hence a contradiction.  Likewise the reverse.  If N(N) does not halt, then N(N) will halt. Therefore, H(p, i) cannot be solved using a universal computation system.

It may seem like we are equivocating on the nature of the functions we are describing, since all of the programs so far have a single input while H() has two inputs.  However, any number of inputs can be encoded onto a single input using delimiters.  Therefore, specifying multiple inputs is just an easier way to write out the function than the required steps for encapsulating the inputs together into a single value.

\section{Turing Oracles as Solutions for Incomputable Problems}

Turing recognized that although the value of H() was not computable, it was in fact a true function of its variables --- that is, for every input set, it yielded a single output.  Thus, the halting problem was a hard problem --- it had a solution, but not one that was easy to determine.  Might there be other problems which are harder?  Might there be problems which require the solution to the halting problem to figure out?  If so, how does one go about reasoning about the computational difficulty of an unsolvable problem?  The answer is in Turing Oracles.  

A Turing Oracle (hereafter just oracle) is a black--box function (i.e. no implementation description is given) which solves an incomputable function and yields its answer in a single step.  An oracle machine is a combination of a normal computational system which also has access to an oracle.  The oracle must be well--defined in its abilities, but if it is, we can use it to reason about the process even if the process as a whole is incomputable.  An oracle machine, then, is a regular machine (i.e. a normal computable function) which is connected to an oracle (i.e. the function has access to an operation which is incomputable).

Alan Turing describes the oracle machine as follows:

\begin{quote}
Let us suppose that we are supplied with some unspecified means of solving number theoretic problems; a kind of oracle as it were.  We will not go any further into the nature of this oracle than to say that it cannot be a machine.  With the help of the oracle we could form a new kind of machine (call them o--machines), having as one of its fundamental processes that of solving a given number theoretic problem \citep{turing_1939}.
\end{quote}

Even though we cannot compute the values of functions based on oracle machines (since they are by definition incomputable), we can reason about which problems are reducible to which oracles.  In other words, if we had an oracle for a given problem, what other problems could be solved?  For instance, there is an incomputable function called Rado’s Sigma Function (affectionately known as the ``busy beaver'' function).  This function says, given n, what is the longest non--infinite output of any program of size n?  This is an incomputable function, but it is computable given an oracle for H().

As we talked about earlier, if dualism is true, then at least some aspects of human cognition that are not computable.  However, given the discussion above, even incomputable functions may be representable if we include oracles in our descriptions of cognition.  Several researchers have previously discussed the possibility that the human mind may be an oracle machine (i.e. \citet{copeland_1998}).  However, none of them have suggested including oracles as a standard part of cognitive modeling, or how one might apply oracles to cognitive modeling.  The goal of this paper is to present both the concept of modeling cognition via oracle machines as well as applying the concept in a model of human problem--solving on insight problems.

\section{Partial Solutions to Incomputable Functions Using Additional Axioms}

Incomputable functions are unpredictably sensitive to initial conditions.  In other words, there is no way to computably predict the difference in behavior of the function from the differences in changes to the initial conditions ahead--of--time.  If you could, they would by definition not be incomputable!  However, one can make partial solutions to these functions by incorporating additional axioms.

An axiom is a truth that is pre--computational.  In other words, it is a truth about computation rather than a result of computation.  Chaitin has shown that one can use additional axioms to make partial solutions of incomputable functions \citep{chaitin_1982}.  For instance, if God were to tell us that there are 30 programs less than size n that halt for a given programming language, then we could use that fact to solve exactly the halting problem for programs less than size n.  This is not a complete solution, but rather a partial solution.  Nonetheless, it is a solution larger than what was originally determinable without the additional axiom.

Now, most axioms don’t come to us like this.  Most axioms would be of the form that says that programs which have a certain pattern of state changes will never halt.  This is not an exclusive list, but the list of additional programs that are now known non--halters through this axiom may be infinitely large.  Therefore, by adding axioms, one could potentially be adding infinite subsets of solutions to incomputable problems.  Axiom addition is also by definition non--algorithmic.  If we could algorithmically add axioms, then the halting problem would be solvable.  Since the halting problem is not solvable algorithmically, then axiom addition is not an algorithmic endeavor.

Once an axiom is known, however, then the computation of halters and non--halters for which we have sufficient axioms is an algorithmic problem.  Therefore, the discovery of new axioms converts subsets of problems from non--algorithmic to algorithmic forms.

\section{Towards Defining a Turing Oracle for Modeling Human Problem--Solving on Insight Problems}

Now that we have spent some time investigating computability theory, we will attempt to relate this theory to problems in cognitive science --- namely problem--solving for insight problems.

Cognitive science usually breaks problem--solving into two broad categories --- analysis problems and insight problems.  Analysis problem are problems which can be solved using a known algorithm or set of known heuristics.  Analysis problems are usually characterized by the subject being aware of how close he is to solving the problem, the benefits of continuous effort, and the use of pre--existing ideas to solve the problem.  Insight problems, on the other hand, are problems which require a reconceptualization of the process in order to solve them \cite[{chronicle_et_al_2004}. 

An example of a classic insight problem is the nine--dot problem.  In short, the problem is to take a 3x3 square of dots, and draw four lines that connect every dot without picking up your pencil.  In order to solve the puzzle, the person must realize that the solution is to extend one of the lines beyond the confines of the box, and make a ``non--dot turn.''  This reconceptualization of the problem is rare, the subject cannot gauge their own progress, and continuous effort is usually not as helpful as taking breaks.

Insight problems like these have significant structural similarity with incomputable functions.  Incomputable functions can be partially solved through adding axioms to the mix.  Axioms function a bit like reconceptualizations --- they allow the problem to be worked from a different angle using a different approach.  Finally, because axioms cannot be generated algorithmically, it makes sense that it would be difficult to tell if one was close to figuring one out.  Likewise, because the person is not following an algorithm (which is impossible for generating an axiom), then continuous effort along the same path is not likely to be helpful.

In research on the nine--dot problem, it has been shown that training on certain ideas such as non--dot turns in similar problems produces an increased success rate on the problem \citep{kershaw_and_ohlsson_2001, kershaw_2004}.  This effectively mirrors the way that axioms function in mathematical problem--solving --- by increasing the number of axioms available to the subject, Kershaw was able to greatly reduce the difficulty of the nine--dot problem for participants.

Because it is mathematically impossible for a person to take an algorithmic approach to the general halting problem, it cannot be classed as an analysis problem.  Because of this, and its many similarities with other insight problems, the halting problem should be classified as an insight problem.  As such, the discoveries that are made for how humans solve the halting problem will help us formulate more generally a theory of human insight.

\section{Human Solutions to the Halting Problem}

As mentioned previously, my contention is that if humans are able to solve incomputable functions, then the physicalism hypothesis is false.\footnote{Penrose and others have suggested that physical processes are non--computational.  However, they do so without a rigorous definition of what counts as ``physical.''  Our goal is to make the definition of physical rigorous enough to be testable, and therefore have used computational tractability as the requirement.  See the final section for additional discussion.}  The halting problem, which is one of the most widely studied incomputable problem, makes a good test case for this idea because of how widely it is studied both on a theoretical and a practical level.

The first bit of evidence we will look at comes from software engineering.  In software development, humans have to develop software programs on universal computation systems, and those programs must halt.  If they do not, their programs will be broken.  Therefore, they must solve problems on at least some subset of the halting problem in order to accomplish their tasks.  In addition, the problems that they are given to solve are not of their own design, so it is not a selection bias.  We can’t say that programmers are only choosing the programs to solve based on their intrinsic abilities to solve them because someone else (usually someone without the computational background needed to know the difference) is assigning the programs.  In addition, we can’t say that programmers are working around their inabilities to solve certain types of halting problems, because, while the programmer might add some extrinsic complexity to a program, the complexity of the problem itself has an intrinsic minimum complexity regarding a given programming language.  Likewise, simply writing it in another language doesn’t help, because there exists a finite--sized transformer from any language to any other language, so being able to solve it in one language is de facto evidence of being able to solve it in another.

So, all of this to say, from the experience of the process of programming, we find evidence that humans are able to at least solve a similar problem to the halting problem.  However, there are some important caveats.

A minor caveat is that machines in real life do not exhibit true universal computation as defined in the abstract.  Universal computation systems have an infinite memory and can run forever without breaking down.  However, there are two primary reasons why this is relatively unimportant to our discussion.  The first is that even with fixed--size memory, the halting problem is still practically intractable.  That is, the reason why fixed--size memories allow the halting problem to be solved is that one could do an exhaustive search of machine states to determine if the machine state contains cycles (i.e. two exactly equivalent machine states) before halting.  If so, then the program will halt.  However, calculating the result of the halting problem even using finite--sized memory would require either enormous amounts of time or memory, on the order of $2^n$, where n is the number of bits in memory.\footnote{As an example, one could solve the halting problem on fixed--size memories using a counter \citep{gurari_1989}.  Since the number of possible machine states is $2^n$, then if one counted machine states, we could determine that it must be an non--halting program if the program performs more than $2^n$ computations.  One could implement a faster way of checking for cycles, but it would generally require $2^n$ amount of memory.}  In addition, the reasoning usually given by programmers as to why something shouldn’t halt is a reasoning about the program akin to a proof, not a description of an exhaustive set of attempts.  If humans are regularly solving the halting problem for a large number of programs, then it is not because they are being aided by fixed--size computer memories.

The main caveat is that there do exist programs (even very short programs) for which humans have not solved the halting problem.  Many open problems in number theory can be quite simply converted into a halting problem, so that the answer to the problem can be solved by knowing whether or not a given computation will halt.  If humans had immediate access to a halting problem oracle, why would these programs give such trouble?

As an example, a perfect number is a number which is equal to the sum of its divisors excluding itself.  For instance, 6 is a perfect number because 1, 2, and 3 are all divisors, and they add up to 6.  It is not known if there are any odd perfect numbers.  A program could be written to search and find an odd perfect number, and halt if it finds one.  Such a program can be fairly simply expressed as:

\begin{verbatim}
function odd_perfect_divisor_exists() {
	var i = 3;

	while(true) { // This means to loop forever unless terminated within the loop      
		var divisors = all_divisors_of(i);
		var divisor_sum = sum(divisors);
		if(divisor_sum == i) {
			return i; // i.e. Halt
		} else {
			i = i + 2; // Go to the next odd number      
		}    
	} 
}
\end{verbatim}

Therefore, if the above program halts then there is an odd perfect number.  If it does not halt then there isn’t one.  However, no human currently knows the answer to this question.  Therefore, whatever it is that humans are doing, it isn’t directly knowing the answer to the halting problem.

Should we then conclude that humans will never be able to solve this problem?  If humans had the same limitations on computation as computers, then we should never be able to solve this (and many other) problems.  However, math and science, as disciplines, assume that unknown problems with definite answers will eventually be knowable.  In other words, the progress of science depends on the ability of humans to be able to solve these problems eventually.

In other words, if this is a fundamental limitation of humans, then the search for more and more mathematical truths may be madness --- we will never be able to know them.  This has led some theorists such as Chaitin to suppose that we should, in some cases, simply assume the truth or falsity of some claims as axioms, even in absence of proofs of their truth \citep{chaitin_2006}.  This seems to be a dangerous road to travel.  Chaitin uses the fact that different geometries can be made from different axioms about the nature of the world to justify the arbitrariness of choosing axioms (in the case of geometry, it is the question of whether parallel lines can intersect).  However, the two cases are not equivalent.  There is a difference between an axiom being unconstrained within a system and unprovable within a system.  If an axiom is unconstrained, then that means that, given the remaining axioms, a fully consistent system can be maintained by multiple choices of axioms.  In other words, either axiom is equally true given the remaining axioms.  If an axiom is constrained but unprovable, that means that the truthfulness of an axiom is dependent on the remaining axioms.  In other words, one axiom is true and another is false given the remaining axioms.  In the case of reasoning about the halting problem, we are dealing entirely with constrained but unprovable axioms.  It might be worthwhile to provisionally accept an axiom and see where it takes you, but it is dangerous to include a provisionally--accepted axiom on equal ground with other types of axioms in formal mathematics.

Another option, however, is that humans are able to incrementally arrive at solutions to halting problems.  This would mean that humans have access to an oracle which is more powerful than finitary computational systems, but less powerful than a halting oracle.

\section{An Oracle for Insight Problems}
Bringsjord has argued for the mind being hyper--computational on the basis of his research into human ability to solve the halting problem.  What his group has argued is that they could always determine the halting problem for Turing machines of size n if they took into account the determination of the halting problem for Turing machines of size $n - 1$ \citep{bringsjord_2006}.

Bringsjord’s group has quite a bit of experience with the halting problem, though it is impossible to tell if his formulation is completely true based on the size of the problem space when n goes beyond 4 --- there are then too many programs for humans to analyze (when n is 5 there are 63,403,380,965,376 programs).  What he found, though, is that his group could formulate halting proofs for programs of size n based on previous patterns which were identified on size $n - 1$.  They used the proofs that they made for size $n - 1$ as a basis for the proofs in programs of size n.  This is itself an interesting result, though it is hard to say that these are necessarily based on program size, since there is nothing in the halting problem that is program--size dependent.  A better interpretation, in my view, is that the proofs were built by introducing constrained axioms.  The larger programs utilized the axioms introduced in smaller programs, but potentially required more axioms to solve.  Therefore, the reason that the programs utilized the smaller programs is because they utilized the axioms introduced there.  As the programs became larger, the number of axioms required to determine a solution grew as well.

This explanation would fit what we are looking for quite well --- it is non--algorithmic (it is determining unprovable axioms), it is incremental (each axioms gives more explanatory power), and it is weaker than a halting oracle.

To put this more formally, let’s define some values: 

\begin{description}
\item{$A$}
the minimum set of axioms required to solve $Q(p, i)$

\item{$Q$}
a decision problem (such as the halting problem)

\item{$p$}
a program

\item{$i$} 
the input to program $p$

\item{$B$}
a set of axioms such that the size of the set of the intersection of A and B is one smaller than A.  In other words, B contains all of the axioms required to solve Q(p, i) except one.
\end{description}

Therefore, I am proposing that human insight is describable by an oracle I such that:

$$ A = I(Q, p, i, B) $$

In other words, if a human is given a decision problem over a certain input, and they know all of the axioms needed to solve the problem except one, human insight will reveal the remaining axiom.  If true, this would explains why insight is both incremental and non--computational.  It goes beyond what is available to computation, but still has prerequisites (in this proposal, I am proposing the prerequisite of having all axioms known except one).  Thus, in the case of finding odd perfect numbers, the problem of finding the solution to the problem is that we do not yet have enough pre--existing axioms yet in order to infer the final axiom.

\section{Problems and Directions}

The main problem with the description as stated is that there are different kinds of axioms, yet there is insufficient mathematical theory (at least known to the author) to differentiate types of axioms.  At present, we should make a distinction between bottom--up and top--down axioms.  As mentioned earlier, if God were to tell us that there are X halting programs of size n, we could determine which ones they were by running all of them simultaneously until X of them halt.  We can consider this kind of axiom a ``top--down'' axiom because it requires prior knowledge of the entire spectrum of the problem to determine.  Another kind of axiom can be considered a ``bottom--up'' axiom.  This is an axiom which requires a minimum of understanding in order to be apprehended, whose truth is knowable even if not provable within its own formalism, and whose application is not intrinsically bounded.  

As an example, we can come up with an axiom which says that if we have a loop in a program whose control variable is monotonically decreasing, and whose termination condition is greater than its start value, that program will never halt.  One can make a simple inductive proof of the statement, and that axiom will then allow us to determine the value of the halting problem for an infinite subset of programs.\footnote{Some may claim that, since it is proved using an inductive proof, this statement becomes a theorem rather than an axiom.  However, it is only a theorem from second--order logic, since general induction requires second--order logic, and can only be imported to first--order logic as an axiom \citep{enderton_2012}.  Since the machine itself is a first--order logic machine \citep{turing_1936}, it is an axiom from the perspective of the first--order system.}  Thus, it acts as a bottom--up axiom.  In addition, as should be obvious, the introduction of such an axiom converts an infinite subset of problems from insight problems to analysis problems.  Since we have the axioms, we can then proceed algorithmically!

So then that leaves us with several open questions:

\begin{itemize}
\item Are there other properties of axioms which are important to the sequence in which they may be found?
\item Are there other prerequisites for finding these axioms?
\item In what ways (if any) do axioms relate to program size?
\item Is there a proper way to measure the size of an axiom?
\end{itemize}

Chaitin’s proposal for measuring axioms is related to his Ω probability.  Ω is the probability for a given Turing machine as to whether or not it will halt, which, for the sake of his theory, is written out as a string of bits.  Chaitin measures axioms by the number of bits of Ω they are able to compute.  If your axiom can deduce two bits of Ω, then the axiom is two bits long \citep{chaitin_2007}.  A naive approach to using this definition might say that humans are able to deduce a single bit of Ω when needed.  However, these bits are much ``smaller'' than the types of axioms that humans tend to develop, which are much wider in extent (each bit of Ω is a single program). There seems to be, based on experience, some intrinsic ordering on the discoverability of axioms present within Ω.  One can define an algorithm to discover 1s (halts) within omega, with an implicit ordering based on length of program and the program’s running time.  For instance, you could write a program which started program 1 at time 1, program 2 at time 2, etc.  Each iteration would run one cycle of each current program, and start one new program.  Each program that halts gives one bit of omega.  Therefore, by exhaustive search, we can find solutions to Ω one bit at a time.  However, this does not match the way in which humans arrive at the solution, which is a more generalized axiomatic approach (covering multiple cases of programs at a time --- not just a single instance like Ω).  Likewise, such algorithms can never discover the 0s (non--halters).   Therefore, although Ω is a powerful conceptualization of the solution to the halting problem, it is unlikely to be helpful in the types of axioms that humans appear to be discovering.

Another possible way to measure the size of an axiom is to measure the size of the recognizer function needed to recognize instances of the axiom.  But again, it is unclear whether or not that would be the measurement which would give us the proper ordering of axiom determination.  It may be harder to implement a recognizer than it is to intuitively recognize an axiom. 

Therefore, in order to proceed further, additional research is needed into the nature of axioms themselves, and different ways that they can be categorized and quantified in order to find a natural sizing and ordering for them.

Then, regarding the embodiment of the oracle itself, we need to look at the following questions:

\begin{itemize}
\item How reliable is the axiom--finding oracle?
\item What are individual differences in this oracle?
\end{itemize}

Such questions will help us understand more about how the oracle interacts with the rest of the mind’s systems.

\section{Generalizing the Oracle Method}

I would like to point out that the main thrust of this paper is not the specific oracle outlined above, though I believe it to be worthy of consideration.  The larger point is that if an operation in the mind is non--physical, this does not preclude it from being modeled.  Specifically, oracles seem to fit the bill for a wide variety of non--physical operations.  There are probably other operations which will require other formalisms, but the point is that we need not shy away from formalisms simply because the formalism isn’t physically computable.  

So how does one make a general application of oracles to modeling the mind?  First of all, it is important that the operation under consideration be well--defined in terms of what it is doing.  It is not worthwhile to simply state ``something occurs here'' --- such is not a well--specified description.  In our example above, we have postulated specific preconditions (a decision problem, a program, its input, and a set of existing axioms) and a specific postcondition (the needed axiom to solve the problem).  One could potentially use Dembski’s concept of specification to determine whether or not the given specification is too broad or if it is reasonably constraining.  Dembski’s measurement is basically a relationship of the potentially--described target states to the specification length.  If a specification doesn’t sufficiently limit the target space of possibilities, it is not a useful specification \citep{dembski_2005}.  

Second, you need to have specific reasons to believe that the proposed process is incomputable.  In our example, since solving the halting problem is known to be incomputable, and adding axioms is incomputable by definition (otherwise they would be theorems), then we have specific evidence that the proposed process is incomputable.  

The hard part then comes in testing the theory.  Because the results are incomputable, and not even likely reducible to a probability distribution, testing it is more difficult.  In the case of computable causes, a specific end--point prediction can be established by computation, and then the result can be validated against that computation.  In this case, the result is not computable, and therefore validation is more difficult.  Validation will often be based on the qualitative description of the process rather than a quantitative prediction.  Parts of it may still be quantifiable, but only with difficulty.  For instance, to test our example, if we had a method of identifying and counting the number of axioms within a person’s mind, we might be able to come up with a quantifiable prediction.  However, since we cannot, we can only test it based on secondary quantifications.  Thus, testability on proposed oracles becomes much more dialectic.

\section{Applications}

This method of using oracles for modeling human cognition has many applications to both psychology and engineering, as well as to the history of technology.  For psychology, it introduces a new way of evaluating mental causes, and a new formalism for modeling and testing them.  In several known cases, human problem solving outperforms what is expected from computationalism.  For example, \citet{dry_et_al_2006} report that human performance on the Traveling Salesman Problem scales linearly with the number of nodes, which far surpasses any computational estimator for the problem.  Therefore, modeling human performance in terms of an oracle machine may allow more accurate predictions of performance.

For engineering, it allows us to better identify and measure complexity.  If axioms become quantifiable, and the number of axioms required to solve problems becomes quantifiable, then this can transform the practice of complexity estimation.  One such method to use these ideas to calculate software complexity is given in \citet{bartlett_2012} later in this volume.  

This idea can also be applied to software computer games.  Many computer games are organized by ``levels'' so that each level is harder than the previous one.  One could use axioms as a measure of hardness, and construct the levels so that each one introduces a new axiom used to complete the game.  This would allow a more rigorous approach to game level design at least in certain types of computer games.

A final way of using this idea is in order to make sense of the history of technology, including science and mathematics.  It has been a curious feature that many ``leaps'' in scientific or mathematical thought have been made simultaneously by multiple people.  Newton and Leibniz both independently invented the calculus, Chaitin and Kolmogorov both independently invented algorithmic information theory, Elisha Gray and Alexander Graham Bell both filed a patent for the telephone on the same day, and the list goes on and on.  This model, if correct, would validate the view of \citet{stokes_1986} that ``even though there is no algorithm of discovery, there are logical elements present in the process whereby a novel hypothesis is devised...''  This model presents a non--algorithmic process, and shows the logical elements which are within its prerequisites.  Therefore, when ideas are widespread, then multiple people will each be a single axiom away from discovery.  Therefore, faced with the same problem, they will both be able to realize the same, missing axiom.

\section{Final Considerations}

I hope to have shown in these pages that there is good evidence human cognition goes beyond what has been traditionally considered as ``physical,'' and that this does not preclude cognitive modeling.  In the first part of this paper ``physical'' was defined to mean ``computable'' in order to avoid the ambiguities of the term.  After all, what would one do if someone simply claimed, for instance, that they think that humans have a separate soul, but simply decided that the soul was physical?  Without a solid definition of what is and is not physical, nothing prevents such a formulation.   

Penrose and Copeland have both made a similar suggestion \citep{copeland_1998, hodges_2000}.  Both have agreed that humans seem to be oracle machines, but think that we are purely physical oracle machines.  However, neither of them provided a sufficient definition of what counted as physical or non--physical to make a proper distinction.  Nothing that either of them has said would contradict what is defended in this paper, though Penrose argues that there is even more to human consciousness than is representable through oracle machines --- a position with which I provisionally agree.   For instance, it is hard to consider the act of true understanding as a process involving numbers at all, as Searle’s Chinese Room argument shows \citep{searle_1980}.

Another possible objection, then, is to say that the universe as a whole isn’t physical.  It could be possible, for instance, that even the fundamental laws of matter are only fully describable using oracles, and none of them at all are computable with finitary methods, and finitary methods can only be used to solve certain macro--systems which are the exception rather than the rule.  However, even if that were true, that would not lead to the conclusion that physicalism is true and that we should simply classify incomputable functions as physical along with computable ones.  Instead it would tell us that the idealists such as \citet{henry_2005}, who believe that the physical is a mere epiphenomenon and the non--physical is what is really real, were the ones who were right all along.  \citet{robertson_1999} commented,

\begin{quote}
The possibility that phenomena exist that cannot be modeled with mathematics may throw an interesting light on Weinberg’s famous comment: ``The more the universe seems comprehensible, the more it seems pointless.'' It might turn out that only that portion of the universe that happens to be comprehensible is also pointless.
\end{quote}

In any case, it may turn out that physicalism isn’t true even for physics!  While I have doubts about this scenario, it is certainly a logical possibility.

I should also point out that the reason I am restricting my modeling to human insight is purely practical --- I know of no way that such insight behavior could be detected or measured on non--human creatures.  I know of no philosophical, theoretical, or theological reason why such processes could not be occurring in other creatures at a much lower level.  There is nothing in this proposal that limits itself either to modeling humans or even only organisms.  It is only that in humans we have the place where restricting reality to only computable processes seem most obviously incorrect.

\bibliographystyle{plain}
\bibliography{Bartlett1Library}
